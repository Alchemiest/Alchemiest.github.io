<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf8" />
    <meta name="viewport" content="initial-scale=1.0, width=device-width" />
    <title>
      
        深度学习——线性神经网络 | 科学炼丹
      
    </title>
    <meta name="description" content="由Hexo及华为云驱动"/>
    <meta name="keywords" content=""/>
    
      <link rel="apple-touch-icon"
            sizes="180x180"
            href="/images/apple-touch-icon.png"/>
    
    
      <link rel="icon"
            type="image/png"
            sizes="32x32"
            href="/images/favicon-32x32.png"/>
    
    
      <link rel="icon"
            type="image/png"
            sizes="16x16"
            href="/images/favicon-16x16.png"/>
    
    
      <link rel="mask-icon"
            href="/images/logo.svg"
            color=""/>
    
    
    <link rel="stylesheet" type="text/css" href="/css/layout.css"/>
    
    
  <link rel="stylesheet" type="text/css" href="/css/post.css"/>
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"/>

  <meta name="generator" content="Hexo 6.3.0"></head>
  <body>
    <div class="head">
      <div class="nav">
        <a href="/" class="nav-logo">
          <img alt="logo" height="60px" width="60px" src="/images/logo.svg" />
        </a>
        <input id="navBtn" type="checkbox"/>
        <div class="nav-menu">
          
            
              <a class="nav-menu-item" href="/ability_DA">数据分析</a>
            
              <a class="nav-menu-item" href="/ability_DL">深度学习</a>
            
              <a class="nav-menu-item" href="/award">学习经历</a>
            
              <a class="nav-menu-item" href="/honor">学术成就</a>
            
          
        </div>
        <label class="nav-btn" for="navBtn"></label>
      </div>
    </div>
    <div class="body">
      
  <article class="post-content">
    <div class="post-inner">
      <div class="post-content__head">
        <div class="post-title">深度学习——线性神经网络</div>
        <div class="post-info">
          
  
    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-tag">#深度学习</a>
  
    <a href="/tags/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-tag">#线性神经网络</a>
  
    <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="post-tag">#损失函数</a>
  
    <a href="/tags/%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" class="post-tag">#小批量随机梯度下降</a>
  
    <a href="/tags/Softmax%E5%9B%9E%E5%BD%92/" class="post-tag">#Softmax回归</a>
  
    <a href="/tags/L2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="post-tag">#L2 损失函数</a>
  
    <a href="/tags/%E7%86%B5/" class="post-tag">#熵</a>
  


          <span class="post-date">2022-02-20</span>
        </div>
      </div>
      <div id="postBody" class="post-content__body--toc">
        <div id="tocAnchor" class="toc-anchor">
          <ol id="toc" class="post-toc">
            
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">理论基础—线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="toc-text">解析解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-text">优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">小批量随机梯度下降</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E5%AE%9E%E8%B7%B5%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">基础实践—线性回归的从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-text">定义优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BF%AB%E9%80%9F%E5%AE%9E%E8%B7%B5%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">快速实践—线性回归的简洁实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%E2%80%94Softmax%E5%9B%9E%E5%BD%92"><span class="toc-text">理论基础—Softmax回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-text">网络架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax%E8%BF%90%E7%AE%97"><span class="toc-text">Softmax运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A0%B7%E6%9C%AC%E7%9A%84%E7%9F%A2%E9%87%8F%E5%8C%96"><span class="toc-text">小批量样本的矢量化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L2-Loss"><span class="toc-text">L2 Loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Huber%E2%80%99s-Robust-Loss"><span class="toc-text">Huber’s Robust Loss</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-text">信息论基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%86%B5"><span class="toc-text">熵</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">Softmax回归的从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-1"><span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89Softmax%E7%AE%97%E6%B3%95%EF%BC%88%E5%AE%9A%E4%B9%89%E5%88%9B%E6%96%B0%E7%AE%97%E6%B3%95%EF%BC%89"><span class="toc-text">定义Softmax算法（定义创新算法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%87%AA%E5%8F%98%E9%87%8F%E5%92%8C%E5%9B%A0%E5%8F%98%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%89"><span class="toc-text">定义模型（定义自变量和因变量之间的关系）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88%E5%AE%9A%E4%B9%89%E6%89%80%E6%9C%89%E7%9A%84%E4%BC%B0%E8%AE%A1%E5%80%BC%E5%92%8C%E5%AE%9E%E9%99%85%E5%80%BC%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E8%B7%9D%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%EF%BC%89"><span class="toc-text">定义损失函数（定义所有的估计值和实际值之间的差距如何计算）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E7%B2%BE%E5%BA%A6"><span class="toc-text">分类精度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax%E7%AE%97%E6%B3%95%E6%98%AF%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E7%A1%AE%E5%AE%9A%E5%AE%9E%E9%99%85%E5%80%BC%E5%92%8C%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%80%BC%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E8%B7%9D%E7%9A%84%EF%BC%8C%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E6%98%AF%E5%9C%A8%E8%AE%AD%E7%BB%83%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%88%A4%E6%96%AD%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%AE%9E%E9%99%85%E5%80%BC%E4%B9%8B%E9%97%B4%E5%B7%AE%E8%B7%9D%E7%9A%84%E5%87%BD%E6%95%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%98%AF%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E9%80%BB%E8%BE%91%E5%85%B3%E7%B3%BB%EF%BC%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E7%A1%AE%E5%AE%9A%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%AE%9E%E9%99%85%E5%80%BC%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E8%B7%9D%E6%98%AF%E5%A4%9A%E5%B0%91%E7%9A%84%EF%BC%8C%E6%AF%94%E7%9A%84%E6%98%AFy%E5%92%8Cy-hat%EF%BC%8C%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%98%AF%E7%A1%AE%E5%AE%9A%E5%9C%A8%E6%9C%89%E8%BF%99%E6%A0%B7%E4%B8%80%E4%B8%AA%E5%B7%AE%E8%B7%9D%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%AF%B9%E8%BF%99%E4%B8%AA%E5%B7%AE%E8%B7%9D%E7%9A%84%E5%80%BC%E6%B1%82%E6%89%80%E6%9C%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E7%9A%84%E5%81%8F%E5%AF%BC%EF%BC%8C%E6%9D%A5%E7%A1%AE%E5%AE%9A%E8%B0%83%E6%95%B4%E5%93%AA%E4%B8%AA%E5%8F%82%E6%95%B0%E8%83%BD%E5%A4%9F%E8%AE%A9%E8%BF%99%E4%B8%AA%E5%B7%AE%E8%B7%9D%E5%8F%98%E5%BE%97%E6%9B%B4%E5%B0%8F%E7%9A%84%E3%80%82%E6%89%80%E4%BB%A5%E8%BF%99%E4%B8%A4%E4%B8%AA%E6%98%AF%E5%89%8D%E5%90%8E%E7%9A%84%E9%80%BB%E8%BE%91%E5%85%B3%E7%B3%BB%E7%9A%84%E3%80%82%E6%89%BE%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0%EF%BC%88%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%89%E6%9C%80%E5%B0%8F%E5%80%BC%E5%AF%B9%E5%BA%94%E7%9A%84%E8%87%AA%E5%8F%98%E9%87%8F%E3%80%82%E5%85%B3%E4%BA%8Eiteration%E5%92%8Cepoch%E5%92%8Cbatchsizebatchsize%E6%98%AF%E6%8A%8A%E5%85%A8%E9%83%A8%E6%A0%B7%E6%9C%AC%E5%88%86%E6%88%90n%E4%B8%AA%E5%B0%8F%E6%89%B9%E6%AC%A1%EF%BC%8C%E7%94%A8%E4%B8%80%E4%B8%AAbatch%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E5%8F%82%E6%95%B0%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%8F%AF%E4%BB%A5%E6%8C%89%E7%85%A7%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%AE%97%E6%B3%95%E6%9B%B4%E6%96%B0%E4%B8%80%E6%AC%A1%E5%8F%82%E6%95%B0%EF%BC%8C%E6%9B%B4%E6%96%B0%E7%9A%84%E8%BF%99%E4%B8%80%E6%AC%A1%E5%8F%82%E6%95%B0%E5%B0%B1%E6%98%AF%E4%B8%80%E4%B8%AAiteration%E3%80%82%E7%94%A8k%E4%B8%AAbatch%E6%9D%A5%E8%BF%9B%E8%A1%8Ck%E6%AC%A1%E8%AE%AD%E7%BB%83%E5%B0%B1%E6%98%AF%E8%BF%9B%E8%A1%8C%E4%BA%86k%E4%B8%AAiteration%E3%80%821%E4%B8%AAepoch%E7%AD%89%E4%BA%8E%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E9%9B%86%E5%BD%93%E4%B8%AD%E7%9A%84%E5%85%A8%E9%83%A8%E6%A0%B7%E6%9C%AC%E8%AE%AD%E7%BB%83%E4%B8%80%E6%AC%A1%EF%BC%8Cm%E4%B8%AAepoch%E5%B0%B1%E6%98%AF%E6%95%B4%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E8%A2%AB%E8%BD%AE%E4%BA%86m%E6%AC%A1%E3%80%82"><span class="toc-text">Softmax算法是定义模型的交叉熵损失函数是确定实际值和模型预测值之间的差距的，是损失函数，是在训练的过程中判断模型与实际值之间差距的函数梯度下降是模型的优化算法逻辑关系：损失函数是确定一个模型和实际值之间的差距是多少的，比的是y和y_hat，优化算法是确定在有这样一个差距之后，对这个差距的值求所有的模型参数的偏导，来确定调整哪个参数能够让这个差距变得更小的。所以这两个是前后的逻辑关系的。找误差函数（损失函数）最小值对应的自变量。关于iteration和epoch和batchsizebatchsize是把全部样本分成n个小批次，用一个batch的数据可以得到一个损失函数和参数之间的关系，也就是可以按照梯度下降的算法更新一次参数，更新的这一次参数就是一个iteration。用k个batch来进行k次训练就是进行了k个iteration。1个epoch等于使用训练集当中的全部样本训练一次，m个epoch就是整个数据集被轮了m次。</span></a></li></ol>
            
          </ol>
        </div>
        
          <div class="post-gallery">
            
          </div>
        
        <p>摘要: 本节内容主要讲述深线性神经网络</p>
<span id="more"></span>
<h2 id="理论基础—线性回归"><a href="#理论基础—线性回归" class="headerlink" title="理论基础—线性回归"></a>理论基础—线性回归</h2><p>线性回归基于几个简单的假设： 首先，假设自变量𝐱x和因变量𝑦y之间的关系是线性的， 即𝑦y可以表示为𝐱x中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。<br>数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一次房屋交易相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。<br>通常，我们使用𝑛来表示数据集中的样本数。 对索引为𝑖的样本，其输入表示为$$𝐱^{(𝑖)}&#x3D;[𝑥^{(𝑖)}_1,𝑥^{(𝑖)}_2]^⊤$$， 其对应的标签是𝑦(𝑖)。</p>
<h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><h3 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h3><ul>
<li>线性回归是对n维输入的加权，外加偏差</li>
<li>使用平方损失来衡量预测值和真实值的差异</li>
<li>线性回归有显式解</li>
<li>线性回归可以看作是单层神经网络</li>
</ul>
<h3 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h3><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><h4 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h4><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。<br>小批量随机梯度下降是三个名词的合体<br>小批量样本的矢量化是为了提高计算效率而采取的对样本的划分策略，小批量是指每次训练并不是用全部的样本进行训练，而是选取一个小批量样本来进行训练，随机是指选取的这个小批量样本是从原样本中随机的选取出来的，梯度下降是指每次迭代通过梯度下降的算法对模型进行优化<br>在每次迭代中，我们首先随机抽样一个小批量$$\mathcal{B}$$，它是由固定数量的训练样本组成的。然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。最后，我们将梯度乘以一个预先确定的正数$$\eta$$，并从当前参数的值中减掉。</p>
<ul>
<li>梯度下降通过不断沿着反梯度方向更新参数求解</li>
<li>小批量随机梯度下降是深度学习默认的求解算法</li>
<li>连个重要的超参数是批量大小和学习率</li>
</ul>
<h2 id="基础实践—线性回归的从零开始实现"><a href="#基础实践—线性回归的从零开始实现" class="headerlink" title="基础实践—线性回归的从零开始实现"></a>基础实践—线性回归的从零开始实现</h2><ul>
<li>epoch：使用训练集的全部数据对模型进行一次完整训练</li>
<li>batch：使用训练集中的一个小部分样本对模型权重进行一次更新，这一小部分样本被称为batch</li>
<li>iteration：使用一个batch的数据对模型进行一次参数更新的过程，这一次训练被称为一次训练<br>第一大步：生成数据集：在本节的实例当中，首先我们假设是存在一个现实当中的客观的规律的，也就是本节当中要学习出来的这个结果，具体实现就是我们按照这个规律来随机生成一组数据集，也就是1000个x样本对应1000个y label，为了大规模的生成样本我们采用面对对象编程，来大批量的生成样本，并且返回我们生成的这个样本。<br>注意：第一大步实际上是Play God的一大步，在这里我们已经知道了实际存在的规律是什么并且按照这个规律用面对对象编程来创建了一个样本，只有在这个最简单的线性回归模型当中我们会这样进行操作，在后面的所有的模型当中就没有这一大环节了，从第一大步之后就是正常的其他的机器学习模型也会进行的正常的步骤<br>第二大步：读取数据集：我们要通过小批量样本来训练我们的模型，因此需要定义一个函数来打乱数据集中的样本然后再以小批量的方式获取数据。具体的实现的方式就是我们创建一个随机读取的函数，随机读取函数其实就是创建一个列表作为这个样本的“index”，然后我们只需要把这个列表进行完全随机的打乱，每次读取的时候按照这个已经打乱了的index对应的读取样本不就等于实现了把样本完全打乱的目标了。<br>第三大步：初始化模型参数：我们只需要随机的给模型一组初始化的参数，通过学习，模型来拟合出最适合当前这组样本的参数，也就是我们要求的模型<br>第四大步：定义模型：跟定义函数一样，定义一个模型<br>第五大步：定义损失函数：需要一个标准来衡量样本值和拟合值的差距<br>第六大步：定义优化算法：需要一个算法来优化模型<br>第七大步：训练模型</li>
</ul>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>首先，我们使用面向对象编程，通过定义一个类来批量的产生一个容量为1000的样本，这个类返回两个变量一个是生成的正态分布的x，另一个是对应的y，通过调用这个函数就可以按照输入的这两个指定的参数生成1000个样本，并将返回的两个变量赋给features和labels这两个变量。</p>
<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。 由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数， 该函数能打乱数据集中的样本并以小批量方式获取数据。<br>要把样本集打乱需要占用大量的内存，所以我们并不打乱原来的样本集，我们只需要定义一个打乱了的minibatch的读取原来样本集的函数就可以了</p>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>[在我们开始用小批量随机梯度下降优化我们的模型参数之前]， (我们需要先有一些参数)。在这里我们相当于随机的找了一组初始化的参数，这里初始化的参数写成多少并不重要，在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。</p>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>接下来，我们必须[定义模型，将模型的输入和参数同模型的输出关联起来。] 回想一下，要计算线性模型的输出， 我们只需计算输入特征𝐗和模型权重𝐰的矩阵-向量乘法后加上偏置𝑏。 注意，上面的𝐗𝐰是一个向量，而𝑏是一个标量。 </p>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>因为需要计算损失函数的梯度，所以我们应该先定义损失函数。这里我们使用平方损失函数。 在实现中，我们需要将真实值y的形状转换为和预测值$$\hat y$$的形状相同。<br>高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。</p>
<h3 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><p>线性回归有解析解。 尽管线性回归有解析解，但本书中的其他模型却没有。 这里我们介绍小批量随机梯度下降。<br>在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。 下面的函数实现小批量随机梯度下降更新。 该函数接受模型参数集合、学习速率和批量大小作为输入。每 一步更新的大小由学习速率lr决定。 因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（batch_size） 来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>现在我们已经准备好了模型训练所有需要的要素，可以实现主要的[训练过程]部分了。 理解这段代码至关重要，因为从事深度学习后， 你会一遍又一遍地看到几乎相同的训练过程。 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法sgd来更新模型参数。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#引入需要的库</span></span><br><span class="line">%matplotlib inline<span class="comment">#%matplotlib inline 可以在Ipython编译器里直接使用，功能是可以内嵌绘图，并且可以省略掉plt.show()这一步。</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))<span class="comment">#三个参数第一个为均值，第二个是方差，第三个是生成的这个张量的大小（长和宽），这里实际表示的是，生成了一个1000行（也就是1000个样本）的，每一行都有2个参数的x的实例，也就是1000行2列的一个张量</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))<span class="comment">#其中这个reshape((-1, 1))代表把y这个张量重整为一个一列的二维数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图观察关系</span></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.scatter(features[:, <span class="number">1</span>].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义小批量随机读取数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)<span class="comment">#features 是1000＊2的矩阵，len()是取其第一维度的大小</span></span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))<span class="comment">#这一步给现在的所有的样本的index创建了一个列表</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)<span class="comment">#这一步把上面创建的这个index的列表打乱了用以实现随机读取</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):<span class="comment">#第三个是步长</span></span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        这里的batch_indices是一个张量，同时，实际上也是一个向量，用torch_tensor创建的一个向量</span></span><br><span class="line"><span class="string">        indices是已经打乱了的原样本集的index的列表</span></span><br><span class="line"><span class="string">        i是range(0, num_examples, batch_size)里面的每一个数字，这每一个数字就是固定间隔的数字，比如1,3,5,7,9或者0,3,6,9等等这种</span></span><br><span class="line"><span class="string">        这里创建的batch_indices实际上就是从原本样本集当中取出来的这个minibatch，它的实际构成是已经打乱了的这个index列表也就是indices列表当中的每第i个到i+步长个前包后不包比如3,4,5这种，这几个连续的index[i: i+步长]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]<span class="comment">#yield 是一个类似 return 的关键字，迭代一次遇到 yield 时就返回 yield 后面的值。重点是：下一次迭代时，从上一次迭代遇到的 yield 后面的代码开始执行。简要理解：yield就是 return 返回一个值，并且记住这个返回的位置，下次迭代就从这个位置后开始。</span></span><br><span class="line">        <span class="comment">#这里这个函数返回的就是从features和labels里面截取出来的minibatch</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)<span class="comment">#因为我们需要计算梯度，故用requires_grad=True来保留梯度特征</span></span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)<span class="comment">#由于需要使用梯度下降的方法不断更新w和b的值，所以我们才要加上 requires_grad=True，因为是True的话，代表告诉系统，需要它去追踪这个量的变化，并不断更新它</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义优化算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span></span><br><span class="line">        <span class="comment"># 并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="快速实践—线性回归的简洁实现"><a href="#快速实践—线性回归的简洁实现" class="headerlink" title="快速实践—线性回归的简洁实现"></a>快速实践—线性回归的简洁实现</h2><h2 id="理论基础—Softmax回归"><a href="#理论基础—Softmax回归" class="headerlink" title="理论基础—Softmax回归"></a>理论基础—Softmax回归</h2><p>Softmax回归实际上是逻辑回归的一个小小的组成部分，逻辑回归就是我们通常所指的分类问题，分类问题当中</p>
<ul>
<li>回归估计一个连续值：单连续值的输出、自然区间、跟真实值的区别作为损失</li>
<li>分类预测一个离散类别：通常多个输出、输出i是预测为第i类的置信度<br>通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题：</li>
</ul>
<ol>
<li>我们只对样本的“硬性”类别感兴趣，即属于哪个类别；</li>
<li>我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。</li>
</ol>
<h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><p>但是一般的分类问题并不与类别之间的自然顺序有关。 幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：独热编码（one-hot encoding）。 独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。</p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的仿射函数（affine function）。 每个输出对应于它自己的仿射函数。<br>在我们的例子中，由于我们有4个特征和3个可能的输出类别，我们将需要12个标量来表示权重（带下标的$$w$$），3个标量来表示偏置（带下标的$$b$$）。</p>
<p>（仿射变换，又称仿射映射，是指在几何中，一个向量空间进行一次线性变换并接上一个平移，变换为另一个向量空间。仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。）<br>权重决定了每个特征对我们预测值的影响。$$b$$称为偏置（bias）、偏移量（offset）或截距（intercept）。偏置是指当所有特征都取值为0时，预测值应该为多少。如果没有偏置项，我们模型的表达能力将受到限制。<br>分类问题实际上就是上一小节当中的线性回归问题的组合。上一节当中的线性回归问题，y为一个标量，方程是一个向量，通过参数、x和偏置所组成的，这一节当中的分类问题，y为一个向量，方程是一个矩阵，通过一个个线性回归的向量所组成的。</p>
<h3 id="Softmax运算"><a href="#Softmax运算" class="headerlink" title="Softmax运算"></a>Softmax运算</h3><p>现在我们将优化参数以最大化观测数据的概率。 为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。<br>然而我们能否将未规范化的预测𝑜直接视作我们感兴趣的输出呢？ 答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。<br>softmax函数将未规范化的预测变换为非负并且总和为1，同时要求模型保持可导。 我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的总和为1，我们再对每个求幂后的结果除以它们的总和。$$\hat{\mathbf{y}} &#x3D; \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j &#x3D; \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$</p>
<h3 id="小批量样本的矢量化"><a href="#小批量样本的矢量化" class="headerlink" title="小批量样本的矢量化"></a>小批量样本的矢量化</h3><p>为了提高计算效率并且充分利用GPU，我们通常会针对小批量数据执行矢量计算。假设我们读取了一个批量的样本$$\mathbf{X}$$，其中特征维度（输入数量）为$$d$$，批量大小为$$n$$。此外，假设我们在输出中有$$q$$个类别。那么小批量特征为$$\mathbf{X} \in \mathbb{R}^{n \times d}$$，权重为$$\mathbf{W} \in \mathbb{R}^{d \times q}$$，偏置为$$\mathbf{b} \in\mathbb{R}^{1\times q}$$。</p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>在模型当中需要一个损失函数来度量预测的效果。高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。<br>softmax函数给出了一个向量$$\hat{\mathbf{y}}$$，我们可以将其视为“对给定任意输入$$\mathbf{x}$$的每个类的条件概率”。例如，$$\hat{y}<em>1$$&#x3D;$$P(y&#x3D;\text{猫} \mid \mathbf{x})$$。假设整个数据集$${\mathbf{X}, \mathbf{Y}}$$具有$$n$$个样本，其中索引$$i$$的样本由特征向量$$\mathbf{x}^{(i)}$$和独热标签向量$$\mathbf{y}^{(i)}$$组成。我们可以将估计值与实际值进行比较：<br>$$P(\mathbf{Y} \mid \mathbf{X}) &#x3D; \prod</em>{i&#x3D;1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).$$<br>根据最大似然估计，我们最大化$$P(\mathbf{Y} \mid \mathbf{X})$$，相当于最小化负对数似然：<br>$$-\log P(\mathbf{Y} \mid \mathbf{X}) &#x3D; \sum_{i&#x3D;1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})<br>&#x3D; \sum_{i&#x3D;1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)})$$<br>其中，对于任何标签$$\mathbf{y}$$和模型预测$$\hat{\mathbf{y}}$$，损失函数为：<br>$$ l(\mathbf{y}, \hat{\mathbf{y}}) &#x3D; - \sum_{j&#x3D;1}^q y_j \log \hat{y}_j<br>$$.<br>上述损失函数 通常被称为交叉熵损失（cross-entropy loss）。 由于𝐲是一个长度为𝑞的独热编码向量， 所以除了一个项以外的所有项𝑗都消失了。 由于所有$$\hat y_j$$都是预测的概率，所以它们的对数永远不会大于0。 因此，如果正确地预测实际标签，即如果实际标签𝑃(𝐲∣𝐱)&#x3D;1， 则损失函数不能进一步最小化。 注意，这往往是不可能的。<br>除了交叉熵损失函数之外，还有其他的众多损失函数：</p>
<h4 id="L2-Loss"><a href="#L2-Loss" class="headerlink" title="L2 Loss"></a>L2 Loss</h4><p>L2 Loss就是我们常说的均方损失函数$$l(y,y’)&#x3D;\frac{1}{2}(y-y’)^2$$<br>明白一个事情，L1是绝对值偏差，也就是估计值和实际值之间直接做个差就是L1（L就是loss），L2是绝对值偏差的平方（loss的平方）的平均，用L2做损失函数就是L2 loss，用L2做为正则项就是L2正则，L2和后面的那个词是分开的，只是组合在一起</p>
<h4 id="Huber’s-Robust-Loss"><a href="#Huber’s-Robust-Loss" class="headerlink" title="Huber’s Robust Loss"></a>Huber’s Robust Loss</h4><h3 id="信息论基础知识"><a href="#信息论基础知识" class="headerlink" title="信息论基础知识"></a>信息论基础知识</h3><h4 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h4><p>信息论的核心思想是量化数据中的信息内容。在信息论中，该数值被称为分布$$P$$的熵（entropy）。可以通过以下方程得到：<br>$$H[P] &#x3D; \sum_j - P(j) \log P(j)<br>$$</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Softmax回归是一个多类分类模型</li>
<li>使用Softmax操作得到每个子类的预测置信度</li>
<li>使用交叉熵来衡量预测和标号的区别</li>
</ul>
<h2 id="Softmax回归的从零开始实现"><a href="#Softmax回归的从零开始实现" class="headerlink" title="Softmax回归的从零开始实现"></a>Softmax回归的从零开始实现</h2><h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><h3 id="定义Softmax算法（定义创新算法）"><a href="#定义Softmax算法（定义创新算法）" class="headerlink" title="定义Softmax算法（定义创新算法）"></a>定义Softmax算法（定义创新算法）</h3><h3 id="定义模型（定义自变量和因变量之间的关系）"><a href="#定义模型（定义自变量和因变量之间的关系）" class="headerlink" title="定义模型（定义自变量和因变量之间的关系）"></a>定义模型（定义自变量和因变量之间的关系）</h3><h3 id="定义损失函数（定义所有的估计值和实际值之间的差距如何计算）"><a href="#定义损失函数（定义所有的估计值和实际值之间的差距如何计算）" class="headerlink" title="定义损失函数（定义所有的估计值和实际值之间的差距如何计算）"></a>定义损失函数（定义所有的估计值和实际值之间的差距如何计算）</h3><h3 id="分类精度"><a href="#分类精度" class="headerlink" title="分类精度"></a>分类精度</h3><h2 id="Softmax算法是定义模型的交叉熵损失函数是确定实际值和模型预测值之间的差距的，是损失函数，是在训练的过程中判断模型与实际值之间差距的函数梯度下降是模型的优化算法逻辑关系：损失函数是确定一个模型和实际值之间的差距是多少的，比的是y和y-hat，优化算法是确定在有这样一个差距之后，对这个差距的值求所有的模型参数的偏导，来确定调整哪个参数能够让这个差距变得更小的。所以这两个是前后的逻辑关系的。找误差函数（损失函数）最小值对应的自变量。关于iteration和epoch和batchsizebatchsize是把全部样本分成n个小批次，用一个batch的数据可以得到一个损失函数和参数之间的关系，也就是可以按照梯度下降的算法更新一次参数，更新的这一次参数就是一个iteration。用k个batch来进行k次训练就是进行了k个iteration。1个epoch等于使用训练集当中的全部样本训练一次，m个epoch就是整个数据集被轮了m次。"><a href="#Softmax算法是定义模型的交叉熵损失函数是确定实际值和模型预测值之间的差距的，是损失函数，是在训练的过程中判断模型与实际值之间差距的函数梯度下降是模型的优化算法逻辑关系：损失函数是确定一个模型和实际值之间的差距是多少的，比的是y和y-hat，优化算法是确定在有这样一个差距之后，对这个差距的值求所有的模型参数的偏导，来确定调整哪个参数能够让这个差距变得更小的。所以这两个是前后的逻辑关系的。找误差函数（损失函数）最小值对应的自变量。关于iteration和epoch和batchsizebatchsize是把全部样本分成n个小批次，用一个batch的数据可以得到一个损失函数和参数之间的关系，也就是可以按照梯度下降的算法更新一次参数，更新的这一次参数就是一个iteration。用k个batch来进行k次训练就是进行了k个iteration。1个epoch等于使用训练集当中的全部样本训练一次，m个epoch就是整个数据集被轮了m次。" class="headerlink" title="Softmax算法是定义模型的交叉熵损失函数是确定实际值和模型预测值之间的差距的，是损失函数，是在训练的过程中判断模型与实际值之间差距的函数梯度下降是模型的优化算法逻辑关系：损失函数是确定一个模型和实际值之间的差距是多少的，比的是y和y_hat，优化算法是确定在有这样一个差距之后，对这个差距的值求所有的模型参数的偏导，来确定调整哪个参数能够让这个差距变得更小的。所以这两个是前后的逻辑关系的。找误差函数（损失函数）最小值对应的自变量。关于iteration和epoch和batchsizebatchsize是把全部样本分成n个小批次，用一个batch的数据可以得到一个损失函数和参数之间的关系，也就是可以按照梯度下降的算法更新一次参数，更新的这一次参数就是一个iteration。用k个batch来进行k次训练就是进行了k个iteration。1个epoch等于使用训练集当中的全部样本训练一次，m个epoch就是整个数据集被轮了m次。"></a>Softmax算法是定义模型的<br>交叉熵损失函数是确定实际值和模型预测值之间的差距的，是损失函数，是在训练的过程中判断模型与实际值之间差距的函数<br>梯度下降是模型的优化算法<br>逻辑关系：损失函数是确定一个模型和实际值之间的差距是多少的，比的是y和y_hat，优化算法是确定在有这样一个差距之后，对这个差距的值求所有的模型参数的偏导，来确定调整哪个参数能够让这个差距变得更小的。所以这两个是前后的逻辑关系的。找误差函数（损失函数）最小值对应的自变量。<br>关于iteration和epoch和batchsize<br>batchsize是把全部样本分成n个小批次，用一个batch的数据可以得到一个损失函数和参数之间的关系，也就是可以按照梯度下降的算法更新一次参数，更新的这一次参数就是一个iteration。用k个batch来进行k次训练就是进行了k个iteration。1个epoch等于使用训练集当中的全部样本训练一次，m个epoch就是整个数据集被轮了m次。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#引入需要的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据，确定batch大小</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">原始数据集中的每个样本都是28×2828×28的图像。 在本节中，我们[将展平每个图像，把它们看作长度为784的向量。] </span></span><br><span class="line"><span class="string">因为我们的数据集有10个类别，所以网络输出维度为10。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义Softmax操作</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">这个实际上就是softmax的模型了，模型返回的是Softmax（xw+b）（都是向量或者矩阵哈）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">交叉熵损失中yi向量中除了真值==1，其他都是0，所以这里直接算真值对应的预测概率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这里回答一个一直困扰自己的问题，为什么这里能够一下子把交叉熵损失函数精简到这种程度。</span></span><br><span class="line"><span class="string">首先需要明确一个事情，独热向量是每一个样本的label向量，也就是实际值y！而y_hat并不是独热向量！！！！</span></span><br><span class="line"><span class="string">也就是说一个模型可能预测出来某一个样本对每一个类别都有一定的概率是，但是实际上一个样本只能属于一个类别而且这个的概率是1！！！</span></span><br><span class="line"><span class="string">所以这里的交叉熵损失函数并不是特例情况，而是普适情况下的交叉熵损失函数的定义！</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#分类精度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)<span class="comment">#这里是把y_hat的每一行的最大值的值的index重新赋给了y_hat（因为这一行的y之间的不同的自变量x就是index嘛）,然后创造了一个只有一列的y_hat</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y<span class="comment">#这里的cmp是布尔值的类型的向量，这一行就是用来判断这个新创建的只有一列的y_hat和y的对应的实际的值是否都相同，并以布尔值的格式返回并且赋予给cmp</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())<span class="comment">#这里是布尔值类型先转成int类型然后求和，求和后再转成float类型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n<span class="comment">#这里会返回一个长度为n的每一个元素都为0.0的列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):<span class="comment">#当你不确定你的函数里将要传递多少参数时你可以用*args。例如,它可以传递任意数量的参数</span></span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    这里的add函数是什么意思呢，首先，调用Accumulator会创建一个列表，里面包含n个累计的对象，之后这个加法函数</span></span><br><span class="line"><span class="string">    就是按照这个n个要累积的对象和他们要加的元素的值按照一一对应的关系组合成元组并把所有的元组打包成列表，用一个for</span></span><br><span class="line"><span class="string">    把每一组调出来，每一组都是这个对象已经累计的值再加上新要累计的这个值（float格式），并把累计完之后的这个列表重新</span></span><br><span class="line"><span class="string">    赋给Accumulator.data这个属性，然后就完成了累计的操作</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量地绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(</span><br><span class="line">            self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 向图表中添加多个数据点</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br><span class="line"></span><br><span class="line"><span class="comment">#规定学习率和模型优化算法</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br><span class="line"></span><br><span class="line"><span class="comment">#预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>

      </div>
    </div>
  </article>
  <div class="post__foot">
    
      <div class="like-author">
  <input type="checkbox" id="likeCode" />
  <div class="author-face">
    <img height="100px"
         width="100px"
         id="front-face"
         alt="author face"
         src="/images/IMG_5172.JPG# 头像图片"/>
    <img height="100px"
         width="100px"
         id="back-face"
         alt="like code"
         src="/images/IMG_3538.jpg# 支付码图片"/>
  </div>
  <div class="like-text">“请作者喝杯咖啡”</div>
  <label for="likeCode" class="like-btn">
    <svg viewBox="0 0 1024 1024"
         width="20px"
         style="margin-right: 10px"
         height="20px">
      <path d="M466.88 908.96L113.824 563.296a270.08 270.08 0 0 1 0-387.392c108.8-106.56 284.896-106.56 393.696 0 1.504 1.472 2.976 2.944 4.448 4.48 1.472-1.536 2.944-3.008 4.448-4.48 108.8-106.56 284.896-106.56 393.696 0a269.952 269.952 0 0 1 34.016 347.072l-387.392 385.6a64 64 0 0 1-89.92 0.384z" p-id="13650" fill="#ee4242"/>
    </svg>
    喜欢作者
  </label>
</div>

    
    <div class="post-nav">
  
    <a class="post-nav-item-left" href="/2022/02/20/AlexNet/">
      <div class="text-align">
        <svg t="1670570876164"
             class="icon"
             viewBox="0 0 1024 1024"
             width="16"
             height="16">
          <path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="14596"/>
        </svg>
        <span class="text-small">上一篇</span>
      </div>
      <div>深度学习——卷积神经网络——AlexNet</div>
    </a>
  
  <div class="vhr"></div>
  
    <a class="post-nav-item-right" href="/2022/01/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">
      <div class="text-align">
        <span class="text-small">下一篇</span>
        <svg t="1670570876164"
             class="icon"
             viewBox="0 0 1024 1024"
             transform="scale(-1,-1)"
             width="16"
             height="16">
          <path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="14596"/>
        </svg>
      </div>
      深度学习——基础知识
    </a>
  
</div>

    
      <div class="related-post">
  <div class="related__head">
  
    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-tag">#深度学习</a>
  
    <a href="/tags/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-tag">#线性神经网络</a>
  
    <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="post-tag">#损失函数</a>
  
    <a href="/tags/%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" class="post-tag">#小批量随机梯度下降</a>
  
    <a href="/tags/Softmax%E5%9B%9E%E5%BD%92/" class="post-tag">#Softmax回归</a>
  
    <a href="/tags/L2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="post-tag">#L2 损失函数</a>
  
    <a href="/tags/%E7%86%B5/" class="post-tag">#熵</a>
  

</div>
  <div class="realated__body">
    
      <div class="null"><div class="null-item"><div class="null-title"><a href="/2022/06/20/Batch Normalization/" title="深度学习——卷积神经网络——Batch Normalization" rel="bookmark">深度学习——卷积神经网络——Batch Normalization</a></div></div><div class="null-item"><div class="null-title"><a href="/2022/02/20/AlexNet/" title="深度学习——卷积神经网络——AlexNet" rel="bookmark">深度学习——卷积神经网络——AlexNet</a></div></div><div class="null-item"><div class="null-title"><a href="/2021/12/20/LeNet/" title="深度学习——卷积神经网络——LeNet" rel="bookmark">深度学习——卷积神经网络——LeNet</a></div></div><div class="null-item"><div class="null-title"><a href="/2020/07/20/AdaBoost与GBDT/" title="机器学习算法——AdaBoost与GBDT模型" rel="bookmark">机器学习算法——AdaBoost与GBDT模型</a></div></div><div class="null-item"><div class="null-title"><a href="/2021/01/20/K近邻算法/" title="机器学习算法——K近邻算法" rel="bookmark">机器学习算法——K近邻算法</a></div></div></div>
    
  </div>
</div>

    
    
      <div id="gitalk-container"></div>
    
  </div>

    </div>
    <div class="foot">
      <div class="foot-inner">
        <div class="foot__head">
          
            <div class="foot-line">
              <div class="matts">海</div><div class="matts">内</div><div class="matts">存</div><div class="matts">知</div><div class="matts">己</div>
            </div>
          
            <div class="foot-line">
              <div class="matts">天</div><div class="matts">涯</div><div class="matts">若</div><div class="matts">比</div><div class="matts">邻</div>
            </div>
          
        </div>
        <div class="foot__body">
          
            <div class="foot-item">
              <div class="foot-item__head">朋友</div>
              <div class="foot-item__body">
                
                  <div class="text">
                    <img alt="link"
                         height="20px"
                         width="20px"
                         src="/images/icon/icon-link.svg"/>
                    <a class="foot-link" target="_blank" rel="noopener" href="https://github.com/hooozen/hexo-theme-tranquility">Theme Tranquility</a>
                  </div>
                
                <div class="text">
                  <img alt="link"
                       height="20px"
                       width="20px"
                       src="/images/icon/icon-link+.svg"/>
                  <a class="foot-link"
                     href="mailto:jordanlee@stumail.hbu.edu.cn?subject=%E7%94%B3%E8%AF%B7%20Hozen.site%20%E7%9A%84%E5%8F%8B%E9%93%BE%E4%BD%8D%E7%BD%AE">
                  申请友链</a>
                </div>
              </div>
            </div>
          
          
            <div class="foot-item">
              <div class="foot-item__head">账号</div>
              <div class="foot-item__body">
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/images/logo-github.svg"/>
                    <a class="foot-link" target="_blank" rel="noopener" href="https://github.com/Alchemiest">Alchemiest</a>
                  </div>
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/images/logo-wx.svg"/>
                    <a class="foot-link" href="">松果篮</a>
                  </div>
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/images/PNG图像 2.PNG"/>
                    <a class="foot-link" target="_blank" rel="noopener" href="https://b23.tv/qb0poEj">科学炼丹</a>
                  </div>
                
              </div>
            </div>
          
          <div class="foot-item">
            <div class="foot-item__head">联系</div>
            <div class="foot-item__body">
              <div class="text">
                <img alt="link"
                     height="20px"
                     width="20px"
                     src="/images/icon/icon-email.svg"/>
                <a class="foot-link" href="mailto:jordanlee@stumail.hbu.edu.cn">jordanlee@stumail.hbu.edu.cn</a>
              </div>
            </div>
          </div>
        </div>
        <div class="copyright">
          <a href="http://example.com">科学炼丹</a> &nbsp;|&nbsp;由&nbsp;<a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>&nbsp;及&nbsp;
          <svg width="20" height="20" viewBox="0 0 725 725">
            <path fill-rule="evenodd" fill="rgb(221, 221, 221)"
            d="M145.870,236.632 L396.955,103.578 L431.292,419.44 L156.600,522.53 L145.870,236.632 Z" />
            <path fill-rule="evenodd" fill="rgb(159, 159, 159)"
            d="M396.955,103.578 L564.345,234.486 L611.558,513.469 L431.292,419.44 L396.955,103.578 Z" />
            <path fill-rule="evenodd" fill="rgb(0, 0, 0)"
            d="M431.292,419.44 L611.558,513.469 L358.327,595.18 L156.600,522.53 L431.292,419.44 Z" />
          </svg>
          <a target="_blank" rel="noopener" href="https://github.com/hooozen/hexo-theme-tranquility">致远</a>&nbsp;驱动
        </div>
      </div>
    </div>
    
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script type="text/javascript">
  const param = JSON.parse('{"enable":true,"owner":"Alchemiest# MUST HAVE, Your Github Username","admin":null,"repo":"https://github.com/Alchemiest/Alchemiest.github.io# MUST HAVE, The name of the repo you use to store Gitment comments","clientID":"科学炼丹# MUST HAVE, Github client id for the Gitment","clientSecret":null,"distractionFreeMode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN"}')
  param.id = location.pathname
  const gitalk = new Gitalk(param)
  gitalk.render('gitalk-container')
</script>

  
  
    <script type="text/javascript" src=/js/toc.js></script>
  

  </body>
</html>
